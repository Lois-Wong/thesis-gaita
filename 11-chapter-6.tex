\chapter{Discussion} \label{chap:chap-6}
%\subsection{compare with existing solutions}

In evaluating Gaita, we opted not to conduct a quantitative evaluation due to the absence of established baselines or existing systems for evaluating RAG (Retrieval-Augmented Generation) chatbots. Additionally, it would not be logical to compare Gaita against traditional search and recommendation engines, such as course search functionalities on platforms like Coursera, because Gaita operates with a fundamentally different input-output structure. While platforms like Coursera rely on traditional search and popularity-based recommendations, Gaita processes natural language input and tailors recommendations through a conversational interface, which makes a direct comparison inappropriate. \footnotemark[1].\footnotetext[1] {In a prior project, I compared vanilla course search results on Coursera with an search interface that accepted natural language input on the same dataset, and found that the latter offered more context-aware and relevant recommendations \cite{wong_advancing_2024}.}

A second evaluation we considered was to ask users to compare Gaita against other LLM chat systems, such as Gemini and GPT-4 Mini, using Likert scale surveys and metrics like precision at k=3. This evaluation would have included a 3x3 Latin square design to randomize the order of results and ensure balanced comparisons across systems. However, this approach was not carried out because it was not a suitable comparison. While Gaita shares a conversational interface with other LLMs, its iterative prompting feature, which tailors follow-up questions to assess prerequisite knowledge, is difficult to compare. Evaluating each system’s effectiveness would require considering the entire conversation chain, which is challenging due to the complex, dynamic nature of conversational interactions. This iterative nature of Gaita’s prompting is a key component that ensures that the system continuously adapts to the user’s evolving needs, something that static systems or even other LLMs may struggle to achieve. Furthermore, traditional evaluation metrics, such as precision at 3, are designed for static outputs and do not account for the iterative and context-dependent aspects of conversation. This complexity that makes it difficult to apply standard evaluation methods to conversational recommender systems. 

It is important to note that conversational recommender systems (CRS) require an evaluation methodology that accounts for the iterative, context-dependent nature of user interactions, which is often overlooked by traditional evaluation metrics \cite{jannach_evaluating_2023}. Unlike conventional recommenders that provide static lists of recommendations, CRSs interact with users through dynamic dialogues, making it essential to assess not only the accuracy of recommendations but also the overall user satisfaction and engagement. Jannach emphasizes that, given the interactive nature of CRSs, traditional metrics focused on algorithmic accuracy are insufficient on their own. Instead, the author advocates for a mixed-methods approach that combines objective, computational assessments with subjective, perception-oriented evaluations. Jannach argues that effective CRS evaluations must include measures of user satisfaction, engagement, and the quality of the interaction, all of which are integral to understanding how well the system meets user needs and provides value.

Comparing Gaita with standard recommendation systems or LLMs would not account for its unique features, such as tailored follow-up questions to assess prerequisite knowledge. Therefore, a qualitative evaluation focused on user feedback and system usability was more practical and effective for understanding Gaita’s strengths and areas for improvement. While a quantitative approach was considered, qualitative insights offer deeper understanding of Gaita’s user experience, the relevance of its recommendations, and opportunities for refinement in future iterations to better serve learners.

\subsection{Iterative prompting}

To enhance Gaita’s effectiveness, improving the iterative prompting feature is a priority. We plan to introduce a clear stop condition where Gaita stops recommending prerequisite courses once the user is adequately prepared. Additionally, we aim to improve the system’s ability to respond more accurately to user knowledge claims. By improving the system’s understanding of user input, Gaita can offer more relevant course recommendations. Furthermore, the nuance of prerequisite recommendations can be enhanced, ensuring that users are guided through the necessary foundational steps, particularly for more advanced learning paths. 

\subsection{User profiles and progress tracking} 

In order to offer a more personalized and adaptive learning experience, we plan to implement functionality for users to create and maintain a profile, allowing Gaita to store and remember key information about their learning history, preferences, and goals. This profile could be enhanced by incorporating data from surveys or external sources like LinkedIn profiles (with consent), providing more context on the user’s professional background and specific learning needs. By integrating such data, Gaita can offer even more tailored recommendations based on the user’s expertise and career aspirations. Introducing user profiles would enable the system to track a user’s progress over time and adjust learning paths accordingly. For example, if a user finds a particular course too challenging or not engaging enough, Gaita could modify future course recommendations to better align with their current level of understanding and interests. 

\subsection{Integrating with existing platforms}

A key component of expanding Gaita’s impact is integrating it with existing open courseware platforms, such as Coursera and MIT OpenCourseWare. Due to Gaita’s modular design, this integration can be achieved by simply swapping out the database to course offerings from specific platforms or individual universities. Additionally, collaborating with universities to incorporate their course catalogues into Gaita would enable the system to support university students by tailoring learning pathways to their varying needs and goals. Gaita could help bridge gaps in traditional education systems and support students from non-traditional backgrounds. This collaboration could further enable institutions to offer dynamic, personalized learning guidance that serve diverse student populations.

\subsection{Expanding learning resources}

To improve Gaita’s recommendations, expanding the database to include more courses is important. Additionally, broadening the scope of learning resources beyond courses—such as incorporating readings, hands-on projects, and practice exercises—would offer a more holistic and personalized learning experience. By providing a variety of learning methods, Gaita can cater to different learning styles and needs, supporting learners in a more comprehensive manner and enhancing the flexibility of educational pathways it offers.

\subsection{Future evaluation}

As we continue to develop Gaita, future evaluations will focus on obtaining a more diverse sample of user data. This will help to ensure that Gaita’s effectiveness is not limited by a narrow user base, allowing for insights into how it performs across different demographics, learning goals, and educational backgrounds. Additionally, more structured user interviews and feedback will be conducted once Gaita reaches a more refined state. These interviews will help gather deeper insights into the usability of the system and how its recommendations meet the diverse needs of users. As the system gains more traction, quantitative metrics, such as user engagement, completion rates, and possibly A/B testing, will become relevant and can be incorporated into future evaluations. While these aspects are currently out of scope, as the focus was on establishing the system’s foundation, they will be important to consider in future phases of development.
