\chapter{Discussion} \label{chap:chap-6}
%\subsection{compare with existing solutions}

In evaluating Gaita, we opted not to conduct a quantitative evaluation due to the absence of established baselines or existing systems for evaluating RAG (Retrieval-Augmented Generation) chatbots. Additionally, it would not be logical to compare Gaita against traditional search and recommendation engines, such as course search functionalities on platforms like Coursera, because Gaita operates with a fundamentally different input-output structure. While platforms like Coursera rely on traditional search and popularity-based recommendations, Gaita processes natural language input and tailors recommendations through a conversational interface, which makes a direct comparison inappropriate. In a prior project, I compared vanilla course search results on Coursera with an search interface that accepted natural language input on the same dataset, and found that the latter offered more context-aware and relevant recommendations \cite{wong_advancing_2024}.

The second option for comparison was to ask users to evaluate Gaita against other LLM-based systems, such as Gemini and GPT-4 Mini, using Likert scale surveys and metrics like precision at k=3. This evaluation would have included a 3x3 Latin square design to randomize the order of results and ensure balanced comparisons across systems. However, this approach was not carried out because it was not an appropriate comparison. While Gaita shares a conversational interface with other LLMs, its iterative prompting feature, which tailors follow-up questions to assess prerequisite knowledge, is difficult to compare. Evaluating each system’s effectiveness would require considering the entire conversation chain, which is challenging due to the complex, dynamic nature of conversational interactions. However, this iterative nature of Gaita’s prompting is a key component that ensures that the system continuously adapts to the user’s evolving needs, something that static systems or even other LLMs may struggle to achieve. Furthermore, traditional evaluation metrics, such as precision at 3, are designed for static outputs and do not account for the iterative and context-dependent aspects of conversation. This complexity that makes it difficult to apply standard evaluation methods to conversational recommender systems. 

Additionally, as discussed in previous research, conversational recommender systems (CRS) require an evaluation methodology that accounts for the iterative, context-dependent nature of user interactions, which is often overlooked by traditional evaluation metrics \cite{jannach_evaluating_2023}. Unlike conventional recommenders that provide static lists of recommendations, CRSs interact with users through dynamic dialogues, making it essential to assess not only the accuracy of recommendations but also the overall user satisfaction and engagement. Jannach emphasizes that, given the interactive nature of CRSs, traditional metrics focused on algorithmic accuracy are insufficient on their own. Instead, the author advocates for a mixed-methods approach that combines objective, computational assessments with subjective, perception-oriented evaluations. Jannach argues that effective CRS evaluations must include measures of user satisfaction, engagement, and the quality of the interaction, all of which are integral to understanding how well the system meets user needs and provides value.

Comparing Gaita with standard recommendation systems or LLMs would not account for the unique features of Gaita, such as its ability to provide tailored follow-up questions for prerequisite knowledge. Therefore, it became clear that a qualitative evaluation, focused on user feedback and system usability, would better capture the effectiveness and relevance of Gaita’s recommendations.

Ultimately, while a more structured quantitative approach was considered, it was more practical and actionable to focus on qualitative feedback and comparisons with other LLMs to understand the strengths and areas for improvement in Gaita’s personalized recommendations. This qualitative evaluation provides deeper insights into the user experience, the effectiveness of Gaita’s recommendations, and areas where future iterations can be improved to better serve learners.


\section{Enhancements}

\subsection{Iterative Prompting}

\subsection{Integration with Existing Platforms}

\section{Ethical Considerations}
