\chapter{Discussion} \label{chap:chap-6}
%\subsection{compare with existing solutions}

In evaluating Gaita, we opted not to conduct a quantitative evaluation due to the absence of established baselines or existing systems for evaluating RAG (Retrieval-Augmented Generation) chatbots. Additionally, it would not be logical to compare Gaita against traditional search and recommendation engines, such as course search functionalities on platforms like Coursera, because Gaita operates with a fundamentally different input-output structure. While platforms like Coursera rely on traditional search and popularity-based recommendations, Gaita processes natural language input and tailors recommendations through a conversational interface, which makes a direct comparison inappropriate. \footnotemark[1].\footnotetext[1] {In a prior project, I compared vanilla course search results on Coursera with an search interface that accepted natural language input on the same dataset, and found that the latter offered more context-aware and relevant recommendations \cite{wong_advancing_2024}.}

A second evaluation we considered was to ask users to compare Gaita against other LLM chat systems, such as Gemini and GPT-4 Mini, using Likert scale surveys and metrics like precision at k=3. This evaluation would have included a 3x3 Latin square design to randomize the order of results and ensure balanced comparisons across systems. However, this approach was not carried out because it was not a suitable comparison. While Gaita shares a conversational interface with other LLMs, its iterative prompting feature, which tailors follow-up questions to assess prerequisite knowledge, is difficult to compare. Evaluating each system’s effectiveness would require considering the entire conversation chain, which is challenging due to the complex, dynamic nature of conversational interactions. This iterative nature of Gaita’s prompting is a key component that ensures that the system continuously adapts to the user’s evolving needs, something that static systems or even other LLMs may struggle to achieve. Furthermore, traditional evaluation metrics, such as precision at 3, are designed for static outputs and do not account for the iterative and context-dependent aspects of conversation. This complexity that makes it difficult to apply standard evaluation methods to conversational recommender systems. 

It is important to note that conversational recommender systems (CRS) require an evaluation methodology that accounts for the iterative, context-dependent nature of user interactions, which is often overlooked by traditional evaluation metrics \cite{jannach_evaluating_2023}. Unlike conventional recommenders that provide static lists of recommendations, CRSs interact with users through dynamic dialogues, making it essential to assess not only the accuracy of recommendations but also the overall user satisfaction and engagement. Jannach emphasizes that, given the interactive nature of CRSs, traditional metrics focused on algorithmic accuracy are insufficient on their own. Instead, the author advocates for a mixed-methods approach that combines objective, computational assessments with subjective, perception-oriented evaluations. Jannach argues that effective CRS evaluations must include measures of user satisfaction, engagement, and the quality of the interaction, all of which are integral to understanding how well the system meets user needs and provides value.

Comparing Gaita with standard recommendation systems or LLMs would not account for its unique features, such as tailored follow-up questions to assess prerequisite knowledge. Therefore, a qualitative evaluation focused on user feedback and system usability was more practical and effective for understanding Gaita’s strengths and areas for improvement. While a quantitative approach was considered, qualitative insights offer deeper understanding of Gaita’s user experience, the relevance of its recommendations, and opportunities for refinement in future iterations to better serve learners.

\section{Enhancements}

\subsection{Iterative Prompting}

\subsection{Integration with Existing Platforms}

\section{Ethical Considerations}
